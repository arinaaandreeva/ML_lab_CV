{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0Hgq8Ejv96BX",
        "7pPBRexYQOV7",
        "9pEIvFfvY_Fn"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4f1b1c83b3c148ce9d6a5abbdd43821d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_086add73712542bea4b92e01b9db050b",
              "IPY_MODEL_629a4d8656dc476993d24d8518a9e00f",
              "IPY_MODEL_cecc7bc64b6b4b0a84ff0f4f94eb42df",
              "IPY_MODEL_ad50a6e539b94da79eecb51e9691cfdf",
              "IPY_MODEL_c4b6477c3cd641a886059ea55a6bb8e6"
            ],
            "layout": "IPY_MODEL_40ce21bff5784b7eb8a58101c9dc192e"
          }
        },
        "086add73712542bea4b92e01b9db050b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b0816af4e084362b7a35e74799afa2e",
            "placeholder": "​",
            "style": "IPY_MODEL_f649743d8a7d474bab1126070fc26914",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "629a4d8656dc476993d24d8518a9e00f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_4c19a757b31343ba8641e4cd0efda13b",
            "placeholder": "​",
            "style": "IPY_MODEL_26653e53a63c4e3996575e02e42e6a50",
            "value": ""
          }
        },
        "cecc7bc64b6b4b0a84ff0f4f94eb42df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_518833db30f640ca9c82df6fdee3d407",
            "placeholder": "​",
            "style": "IPY_MODEL_b414af5060554d498b1f8605c9f8b0fc",
            "value": ""
          }
        },
        "ad50a6e539b94da79eecb51e9691cfdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_6daf5678a2fe45a3b0cdd16668d4752a",
            "style": "IPY_MODEL_ea16ef32a45f48f58df40b6ddd935937",
            "tooltip": ""
          }
        },
        "c4b6477c3cd641a886059ea55a6bb8e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48457316d8094dafb77ed00092b29685",
            "placeholder": "​",
            "style": "IPY_MODEL_b2e28bca22c1433c8789e7d21b69cf0f",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "40ce21bff5784b7eb8a58101c9dc192e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "1b0816af4e084362b7a35e74799afa2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f649743d8a7d474bab1126070fc26914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c19a757b31343ba8641e4cd0efda13b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26653e53a63c4e3996575e02e42e6a50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "518833db30f640ca9c82df6fdee3d407": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b414af5060554d498b1f8605c9f8b0fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6daf5678a2fe45a3b0cdd16668d4752a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea16ef32a45f48f58df40b6ddd935937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "48457316d8094dafb77ed00092b29685": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2e28bca22c1433c8789e7d21b69cf0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arinaaandreeva/ML_lab_CV/blob/main/Lab_CV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "oOrjB4j3J7Ip"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вводим данные из json, который получаем на kaggle\n",
        "\n",
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "import kagglehub\n",
        "kagglehub.login()\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "4f1b1c83b3c148ce9d6a5abbdd43821d",
            "086add73712542bea4b92e01b9db050b",
            "629a4d8656dc476993d24d8518a9e00f",
            "cecc7bc64b6b4b0a84ff0f4f94eb42df",
            "ad50a6e539b94da79eecb51e9691cfdf",
            "c4b6477c3cd641a886059ea55a6bb8e6",
            "40ce21bff5784b7eb8a58101c9dc192e",
            "1b0816af4e084362b7a35e74799afa2e",
            "f649743d8a7d474bab1126070fc26914",
            "4c19a757b31343ba8641e4cd0efda13b",
            "26653e53a63c4e3996575e02e42e6a50",
            "518833db30f640ca9c82df6fdee3d407",
            "b414af5060554d498b1f8605c9f8b0fc",
            "6daf5678a2fe45a3b0cdd16668d4752a",
            "ea16ef32a45f48f58df40b6ddd935937",
            "48457316d8094dafb77ed00092b29685",
            "b2e28bca22c1433c8789e7d21b69cf0f"
          ]
        },
        "id": "lU6J1WpMJfOU",
        "outputId": "62e0c25d-647f-4021-f22a-cd3ab8d04c61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2b1fdaee-2d25-4318-85ed-db156e00b742\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2b1fdaee-2d25-4318-85ed-db156e00b742\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f1b1c83b3c148ce9d6a5abbdd43821d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Загрузка файлов с Kaggle\"\"\"\n",
        "!kaggle competitions download -c cv-lab-dm-24\n",
        "#!unzip cv-lab-dm-24.zip -d data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G64w4xNMIZtX",
        "outputId": "4734b288-7f84-494e-b9f7-adae570a0c81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading cv-lab-dm-24.zip to /content\n",
            " 99% 549M/553M [00:07<00:00, 84.7MB/s]\n",
            "100% 553M/553M [00:07<00:00, 82.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/cv-lab-dm-24.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/data')"
      ],
      "metadata": {
        "id": "DxmevJz-NH36"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Файлы COCO\n",
        "    Их позже подгружают в модель (например, чере torchvision.datasets.CocoDetection)\n",
        "    В json содржится информация о bounding box, area & category_id\n",
        "    Категория (category_id) используется как таргет\n",
        "    \"\"\"\n",
        "import json\n",
        "with open('/content/data/usdc_train.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "with open('/content/data/test_file_names.json', 'r') as f:\n",
        "    data_test = json.load(f)\n",
        "\n",
        "# # Извлечение категорий и изображений\n",
        "# categories = data['categories']  # Список категорий\n",
        "# images = data['images']  # Список изображений\n",
        "# annotations = data['annotations']  # Список аннотаций к изображениям\n",
        "\n",
        "# # Преобразование в DataFrame (если необходимо)\n",
        "# categories_df = pd.DataFrame(categories)\n",
        "# images_df = pd.DataFrame(images)\n",
        "# annotations_df = pd.DataFrame(annotations)\n"
      ],
      "metadata": {
        "id": "omgkJUczaKvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 1.\n",
        "### Данные без нормализации"
      ],
      "metadata": {
        "id": "LjGUIW-raHDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Загрузка данных без обработки\n",
        "    Нужно раскомментировать код, чтобы загрузилось\"\"\"\n",
        "def load_images(folder_path):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.jpg'): # у нас файлы jpg\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "            img = cv2.imread(filepath) # пиксели имеют тип uint8 (целое число от 0 до 255)\n",
        "            if img is not None:\n",
        "                images.append((filename, img))\n",
        "    return images\n",
        "\n",
        "\n",
        "# Загружаем изображения из папок\n",
        "train_path = \"/content/data/train_images/train_images\"\n",
        "test_path = '/content/data/test_images/test_images'\n",
        "#train_images = load_images(train_path)\n",
        "#test_images = load_images(test_path)"
      ],
      "metadata": {
        "id": "DyWBNu4iI6SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Local Contrast Normalization (LCN)"
      ],
      "metadata": {
        "id": "y6RBxQaIaLGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def local_contrast_normalization(img, kernel_size=8):\n",
        "    \"\"\"\n",
        "    Выполняет Local Contrast Normalization (LCN).\n",
        "    kernel_size: Размер локального окна (например, 8x8).\n",
        "    \"\"\"\n",
        "    # Преобразуем изображение в float32\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "\n",
        "    # Вычисляем локальное среднее значение\n",
        "    mean = cv2.blur(img, (kernel_size, kernel_size))\n",
        "\n",
        "    # Вычисляем локальное стандартное отклонение\n",
        "    squared_diff = cv2.blur((img - mean) ** 2, (kernel_size, kernel_size))\n",
        "    std = np.sqrt(squared_diff+ 1e-5) # добавляем епсилон, чтобы не было деления на 0\n",
        "\n",
        "    # Применяем нормализацию\n",
        "    normalized_img = (img - mean) / std\n",
        "\n",
        "    return normalized_img\n",
        "\n",
        "\"\"\"\n",
        "    Применяет LCN ко всем изображениях в указанной папке.\n",
        "    Сохраняет обработанные изображения в указанную выходную папку.\n",
        "    \"\"\"\n",
        "\n",
        "def apply_lcn_to_images(folder_path, output_folder):\n",
        "\n",
        "    # Проверяем, существует ли выходная папка, если нет — создаем её\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Обрабатываем все изображения в папке\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.jpg'):  # Обрабатываем только файлы .jpg\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "            img = cv2.imread(filepath)  # Загружаем изображение (тип uint8)\n",
        "\n",
        "            if img is not None:\n",
        "                # Преобразуем в grayscale (если изображение цветное)\n",
        "                if len(img.shape) == 3:\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                # Применяем Local Contrast Normalization\n",
        "                normalized_img = local_contrast_normalization(img)\n",
        "\n",
        "                # Преобразуем изображение обратно в uint8 для сохранения\n",
        "                normalized_img_uint8 = np.clip(normalized_img * 255, 0, 255).astype(np.uint8)\n",
        "\n",
        "                # Сохраняем результат в выходную папку с оригинальным именем\n",
        "                output_path = os.path.join(output_folder, filename)\n",
        "                cv2.imwrite(output_path, normalized_img_uint8)\n",
        "\n",
        "output_folder = '/content/data/lcn_images'  # Папка для сохранения обработанных изображений\n",
        "\n",
        "apply_lcn_to_images(train_path, output_folder)"
      ],
      "metadata": {
        "id": "T1WBxvHrKbkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Через torch, x- изображение\"\"\"\n",
        "# def local_contrast_normalization(x, kernel_size=7):\n",
        "#     x = x.float()\n",
        "#     # Локальное среднее\n",
        "#     mean = F.avg_pool2d(x, kernel_size, stride=1, padding=kernel_size//2)\n",
        "#     # Локальное стандартное отклонение\n",
        "#     squared_diff = F.avg_pool2d((x - mean) ** 2, kernel_size, stride=1, padding=kernel_size//2)\n",
        "#     std = torch.sqrt(squared_diff + 1e-5)  # Добавляем небольшой эпсилон для устойчивости\n",
        "\n",
        "#     return (x - mean) / std\n"
      ],
      "metadata": {
        "id": "7DGG00gSaTMf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "dd01155a-650f-4c40-fbbd-2a6e9297b2d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Через torch, x- изображение'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Local response normalization"
      ],
      "metadata": {
        "id": "0Hgq8Ejv96BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def local_response_normalization(x, radius=5, alpha=1.0, beta=0.75, k=2.0):\n",
        "    \"\"\"\n",
        "    Применяет Local Response Normalization (LRN) к тензору x.\n",
        "\n",
        "    :param x: Входной тензор (например, изображение).\n",
        "    :param radius: Радиус локальной нормализации.\n",
        "    :param alpha: Коэффициент масштабирования.\n",
        "    :param beta: Экспоненциальный коэффициент.\n",
        "    :param k: Смещение для улучшения устойчивости.\n",
        "    :return: Нормализованный тензор.\n",
        "    \"\"\"\n",
        "    x = x.float()\n",
        "\n",
        "    # Размер ядра для свертки (с учетом радиуса)\n",
        "    kernel_size = 2 * radius + 1\n",
        "\n",
        "    # Создание весов для свертки\n",
        "    weights = torch.ones(x.size(1), 1, kernel_size, kernel_size).to(x.device)  # Один вес для каждого канала\n",
        "\n",
        "    # Процесс локальной нормализации (свёртка с квадратами значений)\n",
        "    square_sum = F.conv2d(x**2, weight=weights, stride=1, padding=radius, groups=x.size(1))\n",
        "\n",
        "    # Нормализация\n",
        "    norm = (alpha * square_sum + k) ** beta\n",
        "    return x / norm"
      ],
      "metadata": {
        "id": "1eUbpBCb95rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalization_lrn(folder_path, output_folder, radius=5, alpha=1.0, beta=0.75, k=2.0):\n",
        "    \"\"\"\n",
        "    Обрабатывает все изображения в папке и применяет Local Response Normalization (LRN),\n",
        "    затем сохраняет результат в другую папку.\n",
        "\n",
        "    :param folder_path: Путь к папке с изображениями.\n",
        "    :param output_folder: Путь к папке для сохранения обработанных изображений.\n",
        "    :param radius: Радиус для LRN.\n",
        "    :param alpha: Коэффициент масштабирования.\n",
        "    :param beta: Экспоненциальный коэффициент.\n",
        "    :param k: Смещение для LRN.\n",
        "    \"\"\"\n",
        "    # Создаем папку для вывода, если она не существует\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Трансформатор для преобразования изображения в тензор\n",
        "    transform = transforms.ToTensor()\n",
        "\n",
        "    # Обрабатываем все изображения в папке\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.jpg') or filename.endswith('.png'):  # Убедитесь, что работает с изображениями\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "\n",
        "            # Загружаем изображение\n",
        "            img = Image.open(filepath).convert('RGB')  # Преобразуем в RGB, если изображение не в этом формате\n",
        "\n",
        "            # Преобразуем изображение в тензор\n",
        "            x = transform(img).unsqueeze(0)  # Добавляем размерность для batch_size, т.е. (1, 3, H, W)\n",
        "\n",
        "            # Применяем Local Response Normalization (LRN)\n",
        "            normalized_x = local_response_normalization(x, radius, alpha, beta, k)\n",
        "\n",
        "            # Преобразуем тензор обратно в изображение и сохраняем\n",
        "            normalized_img = normalized_x.squeeze(0).clamp(0, 1).numpy().transpose(1, 2, 0)  # (H, W, C)\n",
        "            normalized_img = (normalized_img * 255).astype('uint8')  # Преобразуем в диапазон 0-255 для сохранения\n",
        "\n",
        "            # Сохраняем обработанное изображение\n",
        "            output_path = os.path.join(output_folder, filename)\n",
        "            cv2.imwrite(output_path, cv2.cvtColor(normalized_img, cv2.COLOR_RGB2BGR))  # Сохраняем в BGR\n",
        "\n",
        "    print(f\"Обработка завершена. Изображения сохранены в {output_folder}\")\n",
        "\n",
        "# output_folder = '/content/data/lrn_images'  # Путь к папке для сохранения нормализованных изображений\n",
        "\n",
        "# normalization_lrn(train_path, output_folder, radius=5, alpha=1.0, beta=0.75, k=2.0)"
      ],
      "metadata": {
        "id": "CCufbYn7AOht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В современных задачах обнаружения объектов на автопилоте LRN не является оптимальным выбором, так как он устарел.когда объекты плохо видны из-за изменений контраста или освещенности, Local Contrast Normalization (LCN) может быть полезным методом. Это может помочь сделать детали объектов более явными и улучшить их обнаружение в условиях темноты, тумана и т.п."
      ],
      "metadata": {
        "id": "6GVbOC3dGB6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# поэтому используем LCN и достаем из папки данные\n",
        "#train_images = load_images('/content/data/lcn_images')"
      ],
      "metadata": {
        "id": "BKuv53iLMOEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform"
      ],
      "metadata": {
        "id": "7pPBRexYQOV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2"
      ],
      "metadata": {
        "id": "mxTRMzcga3A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.ColorJitter(brightness=0.5, hue=0.3),\n",
        "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),#affine\n",
        "    #ToTensorV2(),\n",
        "], bbox_params=A.BboxParams(format='coco'))"
      ],
      "metadata": {
        "id": "U7Gzvdg0hZ9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORK_DIR = '/content/data'"
      ],
      "metadata": {
        "id": "9hHFp0O58CdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(data['annotations'])):\n",
        "  for j in range(2):\n",
        "    if(data['annotations'][i]['bbox'][j] + data['annotations'][i]['bbox'][j+2] >= 512):\n",
        "      data['annotations'][i]['bbox'][j+2] -= 3\n",
        "    if(data['annotations'][i]['bbox'][j] < 0):\n",
        "      data['annotations'][i]['bbox'][j] = 0\n",
        "\n",
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "\n",
        "transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.ColorJitter(brightness=0.5, hue=0.3),\n",
        "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),#affine\n",
        "    A.Resize(224, 224),\n",
        "    #ToTensorV2(),\n",
        "], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels']))\n",
        "\n",
        "\n",
        "transformed = []\n",
        "for annotation in tqdm(data['images']):\n",
        "\n",
        "    image_id = annotation['id']\n",
        "    image_path = os.path.join(WORK_DIR + '/lcn_images', annotation['file_name'])\n",
        "\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    bboxes = []\n",
        "    class_labels = []\n",
        "    for ann in data['annotations']:\n",
        "        if ann['image_id'] == image_id:\n",
        "            bbox = ann['bbox']\n",
        "            bboxes.append(bbox)\n",
        "            class_labels.append(ann['category_id'])\n",
        "\n",
        "    transformed.append(transform(image=image, bboxes=np.array(bboxes), class_labels=class_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRd7ejm86ntM",
        "outputId": "67bff879-7ce2-4683-ea30-dd36dbfa5958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12000/12000 [07:07<00:00, 28.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Модели"
      ],
      "metadata": {
        "id": "y91CTJjS2iS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "eNbmHn_Bmr1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def local_contrast_normalization(img, kernel_size=8, epsilon=1e-5):\n",
        "    \"\"\"\n",
        "    Выполняет Local Contrast Normalization (LCN).\n",
        "    kernel_size: Размер локального окна (например, 8x8).\n",
        "    \"\"\"\n",
        "    # Преобразуем изображение в float32\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "\n",
        "    # Вычисляем локальное среднее значение\n",
        "    mean = cv2.blur(img, (kernel_size, kernel_size))\n",
        "\n",
        "    # Вычисляем локальное стандартное отклонение\n",
        "    squared_diff = cv2.blur((img - mean) ** 2, (kernel_size, kernel_size))\n",
        "    std = np.sqrt(squared_diff+ epsilon) # добавляем епсилон, чтобы не было деления на 0\n",
        "\n",
        "    # Применяем нормализацию\n",
        "    normalized_img = (img - mean) / std\n",
        "\n",
        "    return normalized_img"
      ],
      "metadata": {
        "id": "Hm8RAe5WnP0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create your own model and train it\n",
        "Начали с малого, модель, которая предсказывает только category_id"
      ],
      "metadata": {
        "id": "EgrJN6wr2rGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_annotations(data, image_id):\n",
        "    \"\"\"\n",
        "    Extract class labels for a given image ID from COCO annotations.\n",
        "    \"\"\"\n",
        "    class_labels = []\n",
        "    for annotation in data['annotations']:\n",
        "        if annotation['image_id'] == image_id:\n",
        "            class_labels.append(annotation['category_id'])  # Use category_id as the target\n",
        "    return class_labels\n",
        "\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for image, label in batch:\n",
        "      if label != -1:\n",
        "        images.append(image)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Преобразуем изображения в тензор\n",
        "    images = torch.stack(images, dim=0)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, image_dir, transforms=None, apply_lcn=False):\n",
        "        self.data = data\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "        self.apply_lcn = apply_lcn\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data['images'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_info = self.data['images'][idx]\n",
        "        image_id = image_info['id']\n",
        "        image_path = os.path.join(self.image_dir, image_info['file_name'])\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            raise ValueError(f\"Failed to load image: {image_path}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.apply_lcn:\n",
        "            image = local_contrast_normalization(image)\n",
        "\n",
        "        # Получаем метки классов\n",
        "        class_labels = get_image_annotations(self.data, image_id)\n",
        "\n",
        "        # Для классификации используем только один объект (например, первый)\n",
        "        label = class_labels[0] if class_labels else 12  # -1 для случаев без аннотации\n",
        "\n",
        "        if self.transforms:\n",
        "            transformed = self.transforms(image=image)\n",
        "            image = transformed['image']\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load data from JSON\n",
        "with open('/content/data/usdc_train.json') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open('/content/data/test_file_names.json') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "train_transforms = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.ColorJitter(brightness=0.5, hue=0.3),\n",
        "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
        "    A.Resize(128, 128),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "test_transforms = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = CustomDataset(train_data, \"/content/data/train_images/train_images\", transforms=train_transforms, apply_lcn=True)\n",
        "test_dataset = CustomDataset(test_data, \"/content/data/test_images/test_images\", transforms=test_transforms, apply_lcn=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True,collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False,collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "Gn_oMo5emvaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CustomClassifier, self).__init__()\n",
        "        # Сверточная часть (backbone)\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Уменьшает размер в 2 раза\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Ещё уменьшение размера\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)  # Ещё уменьшение\n",
        "        )\n",
        "\n",
        "        # Классификационная голова\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))  # Сводим к фиксированному размеру\n",
        "        self.classifier = nn.Linear(64, num_classes)  # Выход: количество классов\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        pooled_features = self.global_pool(features).flatten(1)\n",
        "        logits = self.classifier(pooled_features)\n",
        "        return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "puQATaKMmwAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "num_classes = 13  # Укажите количество классов\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CustomClassifier(num_classes=num_classes).to(device)\n",
        "\n",
        "# Оптимизатор и функция потерь\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Обучение\n",
        "def train_model(model, train_loader, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "train_model(model, train_loader, optimizer, num_epochs=5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86u6GjffNkZj",
        "outputId": "996ae161-acbd-466e-8ca9-338e478d1eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [09:45<39:03, 585.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 1.1939\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [19:37<29:27, 589.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/5], Loss: 1.1541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [29:23<19:35, 587.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/5], Loss: 1.1469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [39:11<09:47, 587.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/5], Loss: 1.1413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [48:58<00:00, 587.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/5], Loss: 1.1344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "# save\n",
        "joblib.dump(model, \"model_cv1.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JDwfo3KV1HL",
        "outputId": "26b166df-e09a-4649-c61a-da0e6470fb8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(test_loader)"
      ],
      "metadata": {
        "id": "925KI7xotsQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Модель, которая выводит уже и bbox и class, но попытки неуспешны"
      ],
      "metadata": {
        "id": "ysG8wBrHXqne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "class ObjectDetectionDataset(Dataset):\n",
        "    def __init__(self, annotations_file, image_dir, transforms=None):\n",
        "        with open(annotations_file, 'r') as f:\n",
        "            self.data = json.load(f)  # загружаем аннотации\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data['images'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      image_info = self.data['images'][idx]\n",
        "      image_id = image_info['id']\n",
        "      image_path = os.path.join(self.image_dir, image_info['file_name'])\n",
        "\n",
        "      # Загружаем изображение\n",
        "      image = cv2.imread(image_path)\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "      # Получаем аннотации для изображения\n",
        "      annotations = [anno for anno in self.data['annotations'] if anno['image_id'] == image_id]\n",
        "\n",
        "      bboxes = []\n",
        "      labels = []\n",
        "      for anno in annotations:\n",
        "          bboxes.append(anno['bbox'])  # формат [x, y, w, h]\n",
        "          labels.append(anno['category_id'])\n",
        "\n",
        "      # Преобразуем в numpy массивы\n",
        "      bboxes = np.array(bboxes) if bboxes else np.zeros((0, 4))\n",
        "      labels = np.array(labels) if labels else np.zeros((13, 4))\n",
        "\n",
        "      # Применяем трансформации, если они заданы\n",
        "      if self.transforms:\n",
        "          augmented = self.transforms(image=image, bboxes=bboxes, class_labels=labels)\n",
        "          image = augmented['image']\n",
        "          bboxes = augmented['bboxes']\n",
        "          labels = augmented['class_labels']\n",
        "\n",
        "      return image, bboxes, labels\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    images = []\n",
        "    bboxes = []\n",
        "    labels = []\n",
        "\n",
        "    for image, bbox, label in batch:\n",
        "        images.append(image)  # Add image tensor\n",
        "        bboxes.append(torch.tensor(bbox, dtype=torch.float32))  # Add bbox tensor\n",
        "        labels.append(torch.tensor(label, dtype=torch.long))  # Add label tensor\n",
        "\n",
        "    # Stack images into a single tensor [batch_size, C, H, W]\n",
        "    images = torch.stack(images, dim=0)\n",
        "\n",
        "    return images, bboxes, labels\n",
        "\n",
        "train_transforms = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.ColorJitter(brightness=0.5, hue=0.3),\n",
        "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
        "    A.Resize(128, 128),\n",
        "    ToTensorV2()\n",
        "], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels']))\n",
        "\n",
        "# train_transforms = A.Compose([\n",
        "#     A.Resize(224, 224),\n",
        "#     A.HorizontalFlip(p=0.5),\n",
        "#     A.ColorJitter(brightness=0.5, hue=0.3),\n",
        "#     ToTensorV2()  # Ensures the image is converted to a tensor\n",
        "# ]\n"
      ],
      "metadata": {
        "id": "tyYlZ6_BrCTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class ObjectDetectionModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ObjectDetectionModel, self).__init__()\n",
        "\n",
        "        # Используем предобученный ResNet для извлечения признаков (backbone)\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        # Убираем последний слой классификации (FC) и avgpool\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # Убираем avgpool и fc\n",
        "\n",
        "        # Добавляем адаптивный слой пулинга для фиксированного выхода\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))  # Используем глобальный пулинг\n",
        "\n",
        "        # Пример фиктивного входа для определения размерности признаков\n",
        "        dummy_input = torch.zeros(1, 3, 224, 224)  # Имитация входа размером (batch_size, 3, 224, 224)\n",
        "        with torch.no_grad():\n",
        "            dummy_features = self.backbone(dummy_input)  # Получаем фичи после backbone\n",
        "            dummy_features = self.global_pool(dummy_features)  # Применяем глобальный пулинг\n",
        "            dummy_features = dummy_features.view(dummy_features.size(0), -1)  # Преобразуем в (batch_size, feature_dim)\n",
        "            feature_dim = dummy_features.size(1)  # Получаем размерность фичей\n",
        "\n",
        "        # Классификация (для категорий объектов)\n",
        "        self.classifier = nn.Linear(feature_dim, num_classes)\n",
        "\n",
        "        # Регрессия (для bbox)\n",
        "        self.bbox_regressor = nn.Linear(feature_dim, 4)  # 4 координаты для bbox [x, y, w, h]\n",
        "\n",
        "        # Оценка уверенности (objectness score)\n",
        "        self.objectness = nn.Linear(feature_dim, 1)  # для objectness\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Извлечение признаков через backbone\n",
        "        features = self.backbone(x)  # (batch_size, channels, h, w)\n",
        "        features = self.global_pool(features)  # (batch_size, channels, 1, 1)\n",
        "        features = features.view(features.size(0), -1)  # Преобразуем в (batch_size, channels)\n",
        "\n",
        "        # Прогнозы\n",
        "        class_preds = self.classifier(features)  # Прогноз категории\n",
        "        bbox_preds = self.bbox_regressor(features)  # Прогноз bbox\n",
        "        score_preds = torch.sigmoid(self.objectness(features))  # Прогноз уверенности (sigmoid для значения от 0 до 1)\n",
        "\n",
        "        return class_preds, bbox_preds, score_preds\n"
      ],
      "metadata": {
        "id": "eIJ9foxpq-re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Настроим устройство\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Инициализация модели\n",
        "num_classes = 13  # Общее количество классов (включая фон)\n",
        "model = ObjectDetectionModel(num_classes=num_classes).to(device)\n",
        "\n",
        "# Определим функции потерь\n",
        "criterion_class = nn.CrossEntropyLoss()  # Для классификации\n",
        "criterion_bbox = nn.SmoothL1Loss()  # Для регрессии bbox\n",
        "criterion_obj = nn.BCELoss()  # Для objectness score\n",
        "\n",
        "# Оптимизатор\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Датасет и DataLoader\n",
        "train_dataset = ObjectDetectionDataset(\n",
        "    annotations_file='/content/data/usdc_train.json',\n",
        "    image_dir= '/content/data/train_images/train_images', #'/content/data/lcn_images',\n",
        "    transforms=train_transforms\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=custom_collate_fn\n",
        ")\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
        "        running_loss = 0.0\n",
        "        for images, bboxes, labels in train_loader:\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            class_preds, bbox_preds, score_preds = model(images/255)\n",
        "\n",
        "            # Calculate losses\n",
        "            loss_class = 0.0\n",
        "            loss_bbox = 0.0\n",
        "\n",
        "            for i in range(len(bboxes)):\n",
        "                target_bboxes = bboxes[i].to(device)\n",
        "                target_labels = labels[i].to(device)\n",
        "                print(class_preds[i], bbox_preds[i], target_labels)\n",
        "\n",
        "                loss_class += criterion_class(class_preds[i], target_labels)\n",
        "                loss_bbox += criterion_bbox(bbox_preds[i], target_bboxes)\n",
        "\n",
        "            # Normalize the loss by batch size\n",
        "            loss_class /= len(bboxes)\n",
        "            loss_bbox /= len(bboxes)\n",
        "            loss = loss_class + loss_bbox\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "\n",
        "# Запуск обучения\n",
        "train_model(model, train_loader, optimizer, num_epochs=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "pp6KgLweq_42",
        "outputId": "bcb1d35c-47da-4128-cff5-dbb7090b1aca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.4885,  0.2034,  0.9696,  0.7145,  0.8864, -0.2270, -0.2932, -1.4934,\n",
            "        -2.1856, -1.9779, -1.4708, -0.6105, -0.0426],\n",
            "       grad_fn=<SelectBackward0>) tensor([ 1.3415, -0.0061, -0.1427,  0.8949], grad_fn=<SelectBackward0>) tensor([], dtype=torch.int64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "size mismatch (got input: [13], target: [0])",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-160a6b45116a>\u001b[0m in \u001b[0;36m<cell line: 95>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# Запуск обучения\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-83-160a6b45116a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mloss_class\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0mloss_bbox\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_bboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1294\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3480\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch (got input: [13], target: [0])"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, image):\n",
        "    model.eval()\n",
        "    image = image.unsqueeze(0).to(device)  # Добавляем размерность для батча\n",
        "    with torch.no_grad():\n",
        "        class_preds, bbox_preds, score_preds = model(image)\n",
        "\n",
        "        # Получаем классы, bbox и уверенность\n",
        "        predicted_class = torch.argmax(class_preds, dim=1)\n",
        "        predicted_bbox = bbox_preds.squeeze(0)  # Для одного изображения\n",
        "        predicted_score = torch.sigmoid(score_preds).squeeze(0)  # Для уверенности\n",
        "\n",
        "        return predicted_class, predicted_bbox, predicted_score\n"
      ],
      "metadata": {
        "id": "44cnx1WWq_vY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}